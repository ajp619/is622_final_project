{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Million Song Database\n",
    "IS622 Final Project  \n",
    "Aaron Palumbo | December 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=http://labrosa.ee.columbia.edu/millionsong/tasteprofile>data</a> are provided by The Echo Nest.\n",
    "\n",
    "From the website:\n",
    "\n",
    "> Welcome to the Taste Profile subset, the official user dataset of the Million Song Dataset.\n",
    "\n",
    "> The Echo Nest is committed to giving back to the research community (for instance by creating the MSD!), and they prove it again by releasing the Taste Profile dataset. The dataset contains real user - play counts from undisclosed partners, all songs already matched to the MSD. if you were looking for the right collaborative filtering dataset with audio features, this might be for you! Plus, you can link that user data to lyrics, tags and Last.fm's similar songs, thus you have many viewpoint for explaining the data.\n",
    "\n",
    "The Million Song Dataset Challenge, B. McFee, T. Bertin-Mahieux, D. Ellis and G. Lanckriet, AdMIRe '12 [pdf][bib]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listening data from EchoNest comes as one big text file. Each line contains three fields: user, song, play count.\n",
    "\n",
    "We can see the file on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 apalumbo apalumbo 2.8G Dec 19  2011 ../data/train_triplets.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls -lh ../data/train_triplets.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can copy this to HDFS with the command line tool:\n",
    "\n",
    "    hdfs dfs -put {{ fileLoc }} {{ fileHDFS }}\n",
    "\n",
    "Here I am using the <a href=\"http://jinja.pocoo.org/Jinja\">Jinja2</a> syntax to reference variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'block_size': 134217728,\n",
       "  'group': 'supergroup',\n",
       "  'kind': 'file',\n",
       "  'last_access': 1450542763,\n",
       "  'last_mod': 1450543445,\n",
       "  'name': u'hdfs://localhost:9000/user/apalumbo/final/train_triplets.txt',\n",
       "  'owner': 'apalumbo',\n",
       "  'path': u'hdfs://localhost:9000/user/apalumbo/final/train_triplets.txt',\n",
       "  'permissions': 420,\n",
       "  'replication': 1,\n",
       "  'size': 3001659271L}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show file in hadoop\n",
    "import pydoop.hdfs as hdfs\n",
    "hdfs.lsl(\"/user/apalumbo/final/train_triplets.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of:\n",
    "\n",
    "* 1,019,318 unique users\n",
    "* 384,546 unique MSD songs\n",
    "* 48,373,586 user - song - play count triplets\n",
    "\n",
    "Our goal is to compare three tools for analyzing this data:\n",
    "\n",
    "* pandas\n",
    "* Spark\n",
    "* Hadoop\n",
    "\n",
    "\n",
    "We will make this comparison based on normal tasks encountered while working with data of this type and try to draw some conclusions about the appropriateness of each of these tools. Obviously, the first criterion we will use in the comparison is the feasibility. Assuming the task is feasible in all three tools we will then move to complexity and time. Complexity will be somewhat subjective while time will be more objective. In our conclusions we will also discuss how will each of these methods scale.\n",
    "\n",
    "> _Notes_\n",
    "> * we will be using Apache Spark 1.5.1\n",
    "* Hadoop 2.7.1 accessed from python with pydoop 1.1.0\n",
    "* pandas 0.17.1\n",
    "* we will exercise the tool sequentially and confirm that memory has been released to ensure the resources of the machine are dedicated to the tool at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset the namespace\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       14361144 kB\n",
      "MemFree:         7252340 kB\n",
      "MemAvailable:   12531740 kB\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat /proc/meminfo | grep Mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyechonest import song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.11 (default, Dec  6 2015 18:08:32)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "spark_home = \"/home/apalumbo/workspace/cuny_msda_is622/spark-1.5.1-bin-hadoop2.6/\"\n",
    "\n",
    "# Path for Spark source folder\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "\n",
    "# Append pyspark to Python Path\n",
    "sys.path.append(spark_home + \"python/\")\n",
    "\n",
    "# Append py4j to Python Path\n",
    "sys.path.append(spark_home + \"python/lib/py4j-0.8.2.1-src.zip\")\n",
    "\n",
    "# Launch Spark\n",
    "execfile(spark_home + \"python/pyspark/shell.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Libraries \n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import IPython.display as dis\n",
    "from pyechonest import config\n",
    "import json\n",
    "\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "# Paths\n",
    "fileHDFS = \"hdfs:///user/apalumbo/final/train_triplets.txt\"\n",
    "# use for testing\n",
    "# fileHDFS = \"hdfs:///user/apalumbo/final/train_triplets_100.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Spark to load the data and look at the first few records, is fast and easy:\n",
    "\n",
    "First we need a split function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitFun(line):\n",
    "    row = []\n",
    "    for field in line.split(\"\\t\"):\n",
    "        try:\n",
    "            row.append(int(field))\n",
    "        except ValueError:\n",
    "            row.append(str(field))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOAKIMP12A8C130995', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOAPDEY12A81C210A9', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBBMDR12A8C13253B', 2],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBFNSP12AF72A0E22', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBFOVM12A58A7D494', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBNZDC12A6D4FC103', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBSUJE12A6D4F8CF5', 2],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBVFZR12A6D4F8AE3', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBXALG12A8C13C108', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBXHDL12A81C204C0', 1]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "songs_spark_ref = sc.textFile(fileHDFS)\n",
    "\n",
    "songs_spark = songs_spark_ref.map(lambda line: splitFun(line))\n",
    "\n",
    "dis.display(songs_spark.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the %%timeit magic to measure how fast this operation is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 57.7 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "songs_spark_ref.map(lambda line: splitFun(line)).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we would like to do is to determine some basic information about the data. We can start with the overall size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sqlcontext provides a nice tool for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a schema and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+---------+\n",
      "|                user|              song|playCount|\n",
      "+--------------------+------------------+---------+\n",
      "|b80344d063b5ccb32...|SOAKIMP12A8C130995|        1|\n",
      "|b80344d063b5ccb32...|SOAPDEY12A81C210A9|        1|\n",
      "|b80344d063b5ccb32...|SOBBMDR12A8C13253B|        2|\n",
      "|b80344d063b5ccb32...|SOBFNSP12AF72A0E22|        1|\n",
      "|b80344d063b5ccb32...|SOBFOVM12A58A7D494|        1|\n",
      "|b80344d063b5ccb32...|SOBNZDC12A6D4FC103|        1|\n",
      "|b80344d063b5ccb32...|SOBSUJE12A6D4F8CF5|        2|\n",
      "|b80344d063b5ccb32...|SOBVFZR12A6D4F8AE3|        1|\n",
      "|b80344d063b5ccb32...|SOBXALG12A8C13C108|        1|\n",
      "|b80344d063b5ccb32...|SOBXHDL12A81C204C0|        1|\n",
      "+--------------------+------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField(\"user\", StringType()), \n",
    "                     StructField(\"song\", StringType()), \n",
    "                     StructField(\"playCount\", IntegerType())])\n",
    "\n",
    "# Convert the RDD to a spark DataFrame\n",
    "sdf = songs_spark.toDF(schema)\n",
    "sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print \"Num Rows: {}\".format(sdf.select(\"user\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print \"Num Unique Users: {}\".format(sdf.select(\"user\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Rows: 48373586\n",
      "Num Unique Users: 1019318\n",
      "Num Unique Songs: 384546\n",
      "CPU times: user 1.08 s, sys: 676 ms, total: 1.76 s\n",
      "Wall time: 35min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print \"Num Unique Songs: {}\".format(sdf.select(\"song\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall size, number of unique users, and number of unique songs are shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Songs\n",
    "\n",
    "One of our objectives with this data set is to build a recommendation engine. One simple way to do this is to simply recommend the most popular artists or songs. Let's see how we do this in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can group by song and sum play counts to get a measure of the most popular songs / artists (this takes about 11.5 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'SOBONKR12A58A7A7E0', 726885],\n",
       " [u'SOAUWYT12A81C206F1', 648239],\n",
       " [u'SOSXLTC12AF72A7F54', 527893],\n",
       " [u'SOFRQTD12A81C233C0', 425463],\n",
       " [u'SOEGIYH12A6D4FC0E3', 389880],\n",
       " [u'SOAXGDH12A8C13F8A1', 356533],\n",
       " [u'SONYKOW12AB01849C9', 292642],\n",
       " [u'SOPUCYA12A8C13A694', 274627],\n",
       " [u'SOUFTBI12AB0183F65', 268353],\n",
       " [u'SOVDSJC12A58A7A271', 244730]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 380 ms, sys: 228 ms, total: 608 ms\n",
      "Wall time: 12min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "groupedSongs = sdf.groupBy('song')\n",
    "songsByPlayCount = groupedSongs.sum('playCount') \\\n",
    "        .sort('sum(playCount)', ascending=False)\n",
    "topSongs = songsByPlayCount.take(10)\n",
    "dis.display([[line[i] for i, j in enumerate(line)] for line in topSongs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(song=u'SOBONKR12A58A7A7E0', sum(playCount)=726885) not found\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\xf6' in position 2: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b2f48dc7a991>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msong\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"artist: {}, title: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martist_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"{} not found\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\xf6' in position 2: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "# Use echonest API to look up user/song information\n",
    "echonestAPI = json.load(open(\"../echonest_info.json\", \"rb\"))\n",
    "config.ECHO_NEST_API_KEY = echonestAPI['api_key']\n",
    "\n",
    "for i in topSongs:\n",
    "    try:\n",
    "        s = song.Song(i.song)\n",
    "        print \"artist: {}, title: {}\".format(s.artist_name, s.title)\n",
    "    except IndexError:\n",
    "        print \"{} not found\".format(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we would do the same tasks with Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear the namespace\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat /proc/meminfo | grep Mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pydoop.hdfs as hdfs\n",
    "import pydoop.mapreduce.api as api\n",
    "import os\n",
    "import sys\n",
    "from pyechonest import song\n",
    "from subprocess import call\n",
    "\n",
    "# Paths\n",
    "fileHDFS = \"hdfs:///user/apalumbo/final/train_triplets.txt\"\n",
    "# use for testing\n",
    "# fileHDFS = \"hdfs:///user/apalumbo/final/train_triplets_100.txt\"\n",
    "fileOutput = \"hdfs:///user/apalumbo/final/hadoop_output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdfs.lsl(fileHDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colnames = [\"user\", \"song\", \"playCount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitFun(line):\n",
    "    row = []\n",
    "    for field in line.split(\"\\t\"):\n",
    "        try:\n",
    "            row.append(int(field))\n",
    "        except ValueError:\n",
    "            row.append(str(field))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hadoop_take(file_path, take_lines):\n",
    "    output = []\n",
    "    i = 0\n",
    "    with hdfs.open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            output.append(line)\n",
    "            i += 1\n",
    "            if i >= take_lines:\n",
    "                break\n",
    "    return output\n",
    "\n",
    "songs_hadoop = hadoop_take(fileHDFS, 10)\n",
    "[splitFun(x) for x in songs_hadoop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "songs_hadoop = hadoop_take(fileHDFS, 10)\n",
    "[splitFun(x) for x in songs_hadoop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is more complicated, it is faster than Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I spent time trying to use the python libraries mrjob and pydoop and was unable to get them functioning. I have not been able to isolate the problem. Instead, I will do the hadoop part in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ----\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "    ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear the namespace\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colnames = [\"user\", \"song\", \"playCount\"]\n",
    "\n",
    "fileLoc  = \"file:///home/apalumbo/is622/final_project/data/train_triplets.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "songs_pandas = pd.read_csv(fileLoc, sep=\"\\t\", header=None, names=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "songs_pandas.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# used to connect a console to the notebook\n",
    "%connect_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
