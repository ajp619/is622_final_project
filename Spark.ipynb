{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Million Song Database\n",
    "IS622 Final Project  \n",
    "Aaron Palumbo | December 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=http://labrosa.ee.columbia.edu/millionsong/tasteprofile>data</a> are provided by The Echo Nest.\n",
    "\n",
    "From the website:\n",
    "\n",
    "> Welcome to the Taste Profile subset, the official user dataset of the Million Song Dataset.\n",
    "\n",
    "> The Echo Nest is committed to giving back to the research community (for instance by creating the MSD!), and they prove it again by releasing the Taste Profile dataset. The dataset contains real user - play counts from undisclosed partners, all songs already matched to the MSD. if you were looking for the right collaborative filtering dataset with audio features, this might be for you! Plus, you can link that user data to lyrics, tags and Last.fm's similar songs, thus you have many viewpoint for explaining the data.\n",
    "\n",
    "The Million Song Dataset Challenge, B. McFee, T. Bertin-Mahieux, D. Ellis and G. Lanckriet, AdMIRe '12 [pdf][bib]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listening data from EchoNest comes as one big text file. Each line contains three fields: user, song, play count.\n",
    "\n",
    "We can see the file on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 apalumbo apalumbo 2.8G Dec 19  2011 ../data/train_triplets.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls -lh ../data/train_triplets.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can copy this to HDFS with the command line tool:\n",
    "\n",
    "    hdfs dfs -put {{ fileLoc }} {{ fileHDFS }}\n",
    "\n",
    "Here I am using the <a href=\"http://jinja.pocoo.org/Jinja\">Jinja2</a> syntax to reference variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'block_size': 134217728,\n",
       "  'group': 'supergroup',\n",
       "  'kind': 'file',\n",
       "  'last_access': 1450645260,\n",
       "  'last_mod': 1450543445,\n",
       "  'name': u'hdfs://localhost:9000/user/apalumbo/final/train_triplets.txt',\n",
       "  'owner': 'apalumbo',\n",
       "  'path': u'hdfs://localhost:9000/user/apalumbo/final/train_triplets.txt',\n",
       "  'permissions': 420,\n",
       "  'replication': 1,\n",
       "  'size': 3001659271L}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show file in hadoop\n",
    "import pydoop.hdfs as hdfs\n",
    "hdfs.lsl(\"/user/apalumbo/final/train_triplets.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of:\n",
    "\n",
    "* 1,019,318 unique users\n",
    "* 384,546 unique MSD songs\n",
    "* 48,373,586 user - song - play count triplets\n",
    "\n",
    "Our goal is to compare three tools for analyzing this data:\n",
    "\n",
    "* pandas\n",
    "* Spark\n",
    "* Hadoop\n",
    "\n",
    "\n",
    "We will make this comparison based on normal tasks encountered while working with data of this type and try to draw some conclusions about the appropriateness of each of these tools. Obviously, the first criterion we will use in the comparison is the feasibility. Assuming the task is feasible in all three tools we will then move to complexity and time. Complexity will be somewhat subjective while time will be more objective. In our conclusions we will also discuss how will each of these methods scale.\n",
    "\n",
    "> _Notes_\n",
    "> * we will be using Apache Spark 1.5.1\n",
    "* Hadoop 2.7.1 accessed from python with pydoop 1.1.0\n",
    "* pandas 0.17.1\n",
    "* we will exercise the tool sequentially and confirm that memory has been released to ensure the resources of the machine are dedicated to the tool at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset the namespace\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       14361144 kB\n",
      "MemFree:         6778668 kB\n",
      "MemAvailable:   12892156 kB\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat /proc/meminfo | grep Mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyechonest import song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.11 (default, Dec  6 2015 18:08:32)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "spark_home = \"/home/apalumbo/workspace/cuny_msda_is622/spark-1.5.1-bin-hadoop2.6/\"\n",
    "\n",
    "# Path for Spark source folder\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "\n",
    "# Append pyspark to Python Path\n",
    "sys.path.append(spark_home + \"python/\")\n",
    "\n",
    "# Append py4j to Python Path\n",
    "sys.path.append(spark_home + \"python/lib/py4j-0.8.2.1-src.zip\")\n",
    "\n",
    "# Launch Spark\n",
    "execfile(spark_home + \"python/pyspark/shell.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Libraries \n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import IPython.display as dis\n",
    "from pyechonest import config\n",
    "import json\n",
    "\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "# Paths\n",
    "fileHDFS = \"hdfs:///user/apalumbo/final/train_triplets.txt\"\n",
    "# use for testing\n",
    "# fileHDFS = \"hdfs:///user/apalumbo/final/train_triplets_100.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Spark to load the data and look at the first few records, is fast and easy:\n",
    "\n",
    "First we need a split function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitFun(line):\n",
    "    row = []\n",
    "    for field in line.split(\"\\t\"):\n",
    "        try:\n",
    "            row.append(int(field))\n",
    "        except ValueError:\n",
    "            row.append(str(field))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOAKIMP12A8C130995', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOAPDEY12A81C210A9', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBBMDR12A8C13253B', 2],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBFNSP12AF72A0E22', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBFOVM12A58A7D494', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBNZDC12A6D4FC103', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBSUJE12A6D4F8CF5', 2],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBVFZR12A6D4F8AE3', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBXALG12A8C13C108', 1],\n",
       " ['b80344d063b5ccb3212f76538f3d9e43d87dca9e', 'SOBXHDL12A81C204C0', 1]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 0 ns, total: 20 ms\n",
      "Wall time: 8.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "songs_spark_ref = sc.textFile(fileHDFS)\n",
    "\n",
    "songs_spark = songs_spark_ref.map(lambda line: splitFun(line))\n",
    "\n",
    "dis.display(songs_spark.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the %%timeit magic to measure how fast this operation is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 54 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "songs_spark_ref.map(lambda line: splitFun(line)).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we would like to do is to determine some basic information about the data. We can start with the overall size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sqlcontext provides a nice tool for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a schema and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+---------+\n",
      "|                user|              song|playCount|\n",
      "+--------------------+------------------+---------+\n",
      "|b80344d063b5ccb32...|SOAKIMP12A8C130995|        1|\n",
      "|b80344d063b5ccb32...|SOAPDEY12A81C210A9|        1|\n",
      "|b80344d063b5ccb32...|SOBBMDR12A8C13253B|        2|\n",
      "|b80344d063b5ccb32...|SOBFNSP12AF72A0E22|        1|\n",
      "|b80344d063b5ccb32...|SOBFOVM12A58A7D494|        1|\n",
      "|b80344d063b5ccb32...|SOBNZDC12A6D4FC103|        1|\n",
      "|b80344d063b5ccb32...|SOBSUJE12A6D4F8CF5|        2|\n",
      "|b80344d063b5ccb32...|SOBVFZR12A6D4F8AE3|        1|\n",
      "|b80344d063b5ccb32...|SOBXALG12A8C13C108|        1|\n",
      "|b80344d063b5ccb32...|SOBXHDL12A81C204C0|        1|\n",
      "+--------------------+------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField(\"user\", StringType()), \n",
    "                     StructField(\"song\", StringType()), \n",
    "                     StructField(\"playCount\", IntegerType())])\n",
    "\n",
    "# Convert the RDD to a spark DataFrame\n",
    "sdf = songs_spark.toDF(schema)\n",
    "sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Rows: 48373586\n",
      "CPU times: user 296 ms, sys: 156 ms, total: 452 ms\n",
      "Wall time: 11min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print \"Num Rows: {}\".format(sdf.select(\"user\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Unique Users: 1019318\n",
      "CPU times: user 308 ms, sys: 160 ms, total: 468 ms\n",
      "Wall time: 11min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print \"Num Unique Users: {}\".format(sdf.select(\"user\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Unique Songs: 384546\n",
      "CPU times: user 240 ms, sys: 228 ms, total: 468 ms\n",
      "Wall time: 11min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print \"Num Unique Songs: {}\".format(sdf.select(\"song\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall size, number of unique users, and number of unique songs are shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Songs\n",
    "\n",
    "One of our objectives with this data set is to build a recommendation engine. One simple way to do this is to simply recommend the most popular artists or songs. Let's see how we do this in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can group by song and sum play counts to get a measure of the most popular songs / artists (this takes about 11.5 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'SOBONKR12A58A7A7E0', 726885],\n",
       " [u'SOAUWYT12A81C206F1', 648239],\n",
       " [u'SOSXLTC12AF72A7F54', 527893],\n",
       " [u'SOFRQTD12A81C233C0', 425463],\n",
       " [u'SOEGIYH12A6D4FC0E3', 389880],\n",
       " [u'SOAXGDH12A8C13F8A1', 356533],\n",
       " [u'SONYKOW12AB01849C9', 292642],\n",
       " [u'SOPUCYA12A8C13A694', 274627],\n",
       " [u'SOUFTBI12AB0183F65', 268353],\n",
       " [u'SOVDSJC12A58A7A271', 244730]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 228 ms, sys: 224 ms, total: 452 ms\n",
      "Wall time: 11min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "groupedSongs = sdf.groupBy('song')\n",
    "songsByPlayCount = groupedSongs.sum('playCount') \\\n",
    "        .sort('sum(playCount)', ascending=False)\n",
    "topSongs = songsByPlayCount.take(10)\n",
    "dis.display([[line[i] for i, j in enumerate(line)] for line in topSongs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(song=u'SOBONKR12A58A7A7E0', sum(playCount)=726885) not found\n",
      "artist: Bjï¿½rk, title: Undo (Live - Vespertine World Tour 2001)\n",
      "artist: Kings of Leon, title: Revelry\n",
      "Row(song=u'SOFRQTD12A81C233C0', sum(playCount)=425463) not found\n",
      "artist: Barry Tuckwell, title: Horn Concerto No. 4 in E Flat, K.495: II. Romance (Andante cantabile)\n",
      "Row(song=u'SOAXGDH12A8C13F8A1', sum(playCount)=356533) not found\n",
      "Row(song=u'SONYKOW12AB01849C9', sum(playCount)=292642) not found\n",
      "artist: Five Iron Frenzy, title: Canada\n",
      "artist: Tub Ring, title: Invalid\n",
      "Row(song=u'SOVDSJC12A58A7A271', sum(playCount)=244730) not found\n"
     ]
    }
   ],
   "source": [
    "# Use echonest API to look up user/song information\n",
    "echonestAPI = json.load(open(\"../echonest_info.json\", \"rb\"))\n",
    "config.ECHO_NEST_API_KEY = echonestAPI['api_key']\n",
    "\n",
    "for i in topSongs:\n",
    "    try:\n",
    "        s = song.Song(i.song)\n",
    "        artist = s.artist_name.encode(\"iso-8859-1\")\n",
    "        title = s.title.encode(\"iso-8859-1\")\n",
    "        print \"artist: {}, title: {}\".format(artist, title)\n",
    "    except IndexError:\n",
    "        print \"{} not found\".format(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"stdin_port\": 47167, \n",
      "  \"ip\": \"127.0.0.1\", \n",
      "  \"control_port\": 59681, \n",
      "  \"hb_port\": 54281, \n",
      "  \"signature_scheme\": \"hmac-sha256\", \n",
      "  \"key\": \"c03260cc-cbc0-4025-81b7-7445f0388067\", \n",
      "  \"shell_port\": 51889, \n",
      "  \"transport\": \"tcp\", \n",
      "  \"iopub_port\": 57747\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> ipython <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> ipython <app> --existing kernel-d5352ed9-84b0-4142-9dc1-14d221b9f601.json \n",
      "or even just:\n",
      "    $> ipython <app> --existing \n",
      "if this is the most recent IPython session you have started.\n"
     ]
    }
   ],
   "source": [
    "# used to connect a console to the notebook\n",
    "%connect_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
