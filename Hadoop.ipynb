{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Million Song Database\n",
    "IS622 Final Project  \n",
    "Aaron Palumbo | December 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=http://labrosa.ee.columbia.edu/millionsong/tasteprofile>data</a> are provided by The Echo Nest.\n",
    "\n",
    "From the website:\n",
    "\n",
    "> Welcome to the Taste Profile subset, the official user dataset of the Million Song Dataset.\n",
    "\n",
    "> The Echo Nest is committed to giving back to the research community (for instance by creating the MSD!), and they prove it again by releasing the Taste Profile dataset. The dataset contains real user - play counts from undisclosed partners, all songs already matched to the MSD. if you were looking for the right collaborative filtering dataset with audio features, this might be for you! Plus, you can link that user data to lyrics, tags and Last.fm's similar songs, thus you have many viewpoint for explaining the data.\n",
    "\n",
    "The Million Song Dataset Challenge, B. McFee, T. Bertin-Mahieux, D. Ellis and G. Lanckriet, AdMIRe '12 [pdf][bib]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listening data from EchoNest comes as one big text file. Each line contains three fields: user, song, play count.\n",
    "\n",
    "We can see the file on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 apalumbo apalumbo 2.8G Dec 19  2011 ../data/train_triplets.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls -lh ../data/train_triplets.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can copy this to HDFS with the command line tool:\n",
    "\n",
    "    hdfs dfs -put {{ fileLoc }} {{ fileHDFS }}\n",
    "\n",
    "Here I am using the <a href=\"http://jinja.pocoo.org/Jinja\">Jinja2</a> syntax to reference variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'block_size': 134217728,\n",
       "  'group': 'supergroup',\n",
       "  'kind': 'file',\n",
       "  'last_access': 1450645260,\n",
       "  'last_mod': 1450543445,\n",
       "  'name': u'hdfs://localhost:9000/user/apalumbo/final/train_triplets.txt',\n",
       "  'owner': 'apalumbo',\n",
       "  'path': u'hdfs://localhost:9000/user/apalumbo/final/train_triplets.txt',\n",
       "  'permissions': 420,\n",
       "  'replication': 1,\n",
       "  'size': 3001659271L}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show file in hadoop\n",
    "import pydoop.hdfs as hdfs\n",
    "hdfs.lsl(\"/user/apalumbo/final/train_triplets.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of:\n",
    "\n",
    "* 1,019,318 unique users\n",
    "* 384,546 unique MSD songs\n",
    "* 48,373,586 user - song - play count triplets\n",
    "\n",
    "Our goal is to compare three tools for analyzing this data:\n",
    "\n",
    "* pandas\n",
    "* Spark\n",
    "* **Hadoop**\n",
    "\n",
    "\n",
    "We will make this comparison based on normal tasks encountered while working with data of this type and try to draw some conclusions about the appropriateness of each of these tools. Obviously, the first criterion we will use in the comparison is the feasibility. Assuming the task is feasible in all three tools we will then move to complexity and time. Complexity will be somewhat subjective while time will be more objective. In our conclusions we will also discuss how will each of these methods scale.\n",
    "\n",
    "> _Notes_\n",
    "> * we will be using Apache Spark 1.5.1\n",
    "* Hadoop 2.7.1 accessed from python with pydoop 1.1.0\n",
    "* pandas 0.17.1\n",
    "* we will exercise the tool sequentially and confirm that memory has been released to ensure the resources of the machine are dedicated to the tool at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we would do the same tasks with Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear the namespace\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat /proc/meminfo | grep Mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pydoop.hdfs as hdfs\n",
    "import pydoop.mapreduce.api as api\n",
    "import os\n",
    "import sys\n",
    "from pyechonest import song\n",
    "from subprocess import call\n",
    "\n",
    "# Paths\n",
    "fileHDFS = \"hdfs:///user/apalumbo/final/train_triplets.txt\"\n",
    "# use for testing\n",
    "# fileHDFS = \"hdfs:///user/apalumbo/final/train_triplets_100.txt\"\n",
    "fileOutput = \"hdfs:///user/apalumbo/final/hadoop_output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdfs.lsl(fileHDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colnames = [\"user\", \"song\", \"playCount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitFun(line):\n",
    "    row = []\n",
    "    for field in line.split(\"\\t\"):\n",
    "        try:\n",
    "            row.append(int(field))\n",
    "        except ValueError:\n",
    "            row.append(str(field))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hadoop_take(file_path, take_lines):\n",
    "    output = []\n",
    "    i = 0\n",
    "    with hdfs.open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            output.append(line)\n",
    "            i += 1\n",
    "            if i >= take_lines:\n",
    "                break\n",
    "    return output\n",
    "\n",
    "songs_hadoop = hadoop_take(fileHDFS, 10)\n",
    "[splitFun(x) for x in songs_hadoop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "songs_hadoop = hadoop_take(fileHDFS, 10)\n",
    "[splitFun(x) for x in songs_hadoop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is more complicated, it is faster than Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I spent time trying to use the python libraries mrjob and pydoop and was unable to get them functioning. I have not been able to isolate the problem. Instead, I will do the hadoop part in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used to connect a console to the notebook\n",
    "%connect_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
